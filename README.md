# oss-llm-security
Curated list of Open Source project focused on LLM security 

## Tools / projects

- [EasyJailbreak](https://github.com/EasyJailbreak/EasyJailbreak) ![Stars](https://img.shields.io/github/stars/EasyJailbreak/EasyJailbreak) - An easy-to-use Python framework to generate adversarial jailbreak prompts.
- [fast-llm-security](https://github.com/ZenGuard-AI/fast-llm-security) ![Stars](https://img.shields.io/github/stars/ZenGuard-AI/fast-llm-security) - The fastest && easiest LLM security and privacy guardrails for GenAI apps.
- [Garak](https://github.com/leondz/garak/) ![Stars](https://img.shields.io/github/stars/leondz/garak) -  LLM vulnerability scanner. garak checks if an LLM can be made to fail in an way we don't want. garak probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know nmap, it's nmap for LLMs.
- [HouYi](https://github.com/LLMSecurity/HouYi) ![Stars](https://img.shields.io/github/stars/LLMSecurity/HouYi) - The automated prompt injection framework for LLM-integrated applications.
- [langkit](https://github.com/whylabs/langkit) ![Stars](https://img.shields.io/github/stars/whylabs/langkit)- An open-source toolkit for monitoring Large Language Models (LLMs). Extracts signals from prompts & responses, ensuring safety & security.
- [llm-attacks](https://github.com/llm-attacks/llm-attacks) ![Stars](https://img.shields.io/github/stars/llm-attacks/llm-attacks) - Universal and Transferable Attacks on Aligned Language Models
- [llm-guard](https://github.com/protectai/llm-guard) ![Stars](https://img.shields.io/github/stars/protectai/llm-guard) - The Security Toolkit for LLM Interactions. LLM Guard by Protect AI is a comprehensive tool designed to fortify the security of Large Language Models (LLMs).
- [llm-security](https://github.com/dropbox/llm-security) ![Stars](https://img.shields.io/github/stars/dropbox/llm-security) - Dropbox LLM Security research code and results. This repository contains scripts and related documentation that demonstrate attacks against large language models using repeated character sequences. These techniques can be used to execute prompt injection on content-constrained LLM queries.
- [llm-security](https://github.com/greshake/llm-security) ![Stars](https://img.shields.io/github/stars/greshake/llm-security)  -  New ways of breaking app-integrated LLMs
- [modelscan](https://github.com/protectai/modelscan) ![Stars](https://img.shields.io/github/stars/protectai/modelscan) - Protection against Model Serialization Attacks
- [Open-Prompt-Injection](https://github.com/liu00222/Open-Prompt-Injection) ![Stars](https://img.shields.io/github/stars/liu00222/Open-Prompt-Injection) - Prompt injection attacks and defenses in LLM-integrated applications
- [plexiglass](https://github.com/safellama/plexiglass) ![Stars](https://img.shields.io/github/stars/safellama/plexiglass) - A toolkit for detecting and protecting against vulnerabilities in Large Language Models (LLMs). 
- [ps-fuzz](https://github.com/prompt-security/ps-fuzz) ![Stars](https://img.shields.io/github/stars/prompt-security/ps-fuzz) - Make your GenAI Apps Safe & Secure ðŸš€ Test & harden your system prompt
- [PurpleLlama](https://github.com/facebookresearch/PurpleLlama) ![Stars](https://img.shields.io/github/stars/facebookresearch/PurpleLlama) - Set of tools to assess and improve LLM security. 
- [promptfoo](https://github.com/promptfoo/promptfoo) ![Stars](https://img.shields.io/github/stars/promptfoo/promptfoo) - LLM red teaming and evaluation framework with modelaudit for scanning ML models for malicious code and backdoors.
- [promptmap](https://github.com/utkusen/promptmap) ![Stars](https://img.shields.io/github/stars/utkusen/promptmap) - automatically tests prompt injection attacks on ChatGPT instances.
- [PyRIT](https://github.com/Azure/PyRIT) ![Stars](https://img.shields.io/github/stars/Azure/PyRIT) - The Python Risk Identification Tool for generative AI (PyRIT) is an open access automation framework to empower security professionals and machine learning engineers to proactively find risks in their generative AI systems.
- [rebuff](https://github.com/protectai/rebuff) ![Stars](https://img.shields.io/github/stars/protectai/rebuff) - LLM Prompt Injection Detector.
- [TrustGate](https://github.com/NeuralTrust/TrustGate) ![Stars](https://img.shields.io/github/stars/NeuralTrust/TrustGate) - LLM & Agent attacks detector - Generative Application Firewall (GAF)
- [vibraniumdome](https://github.com/genia-dev/vibraniumdome) ![Stars](https://img.shields.io/github/stars/genia-dev/vibraniumdome) -  LLM Security Platform.
- [vigil-llm](https://github.com/deadbits/vigil-llm) ![Stars](https://img.shields.io/github/stars/deadbits/vigil-llm) -âš¡ Vigil âš¡ Detect prompt injections, jailbreaks, and other potentially risky Large Language Model (LLM) inputs.

## By [OWASP Top 10 for LLM Applications](https://genai.owasp.org/llm-top-10/)

### LLM01: [Prompt Injection](https://genai.owasp.org/llmrisk/llm01-prompt-injection/)
- [EasyJailbreak](https://github.com/EasyJailbreak/EasyJailbreak) ![Stars](https://img.shields.io/github/stars/EasyJailbreak/EasyJailbreak)
- [fast-llm-security](https://github.com/ZenGuard-AI/fast-llm-security) ![Stars](https://img.shields.io/github/stars/ZenGuard-AI/fast-llm-security)
- [Garak](https://github.com/leondz/garak/) ![Stars](https://img.shields.io/github/stars/leondz/garak)
- [HouYi](https://github.com/LLMSecurity/HouYi) ![Stars](https://img.shields.io/github/stars/LLMSecurity/HouYi)
- [langkit](https://github.com/whylabs/langkit) ![Stars](https://img.shields.io/github/stars/whylabs/langkit)
- [llm-attacks](https://github.com/llm-attacks/llm-attacks) ![Stars](https://img.shields.io/github/stars/llm-attacks/llm-attacks)
- [llm-guard](https://github.com/protectai/llm-guard) ![Stars](https://img.shields.io/github/stars/protectai/llm-guard)
- [llm-security](https://github.com/dropbox/llm-security) ![Stars](https://img.shields.io/github/stars/dropbox/llm-security)
- [Open-Prompt-Injection](https://github.com/liu00222/Open-Prompt-Injection) ![Stars](https://img.shields.io/github/stars/liu00222/Open-Prompt-Injection)
- [plexiglass](https://github.com/safellama/plexiglass) ![Stars](https://img.shields.io/github/stars/safellama/plexiglass)
- [PurpleLlama](https://github.com/facebookresearch/PurpleLlama) ![Stars](https://img.shields.io/github/stars/facebookresearch/PurpleLlama)
- [ps-fuzz](https://github.com/prompt-security/ps-fuzz) ![Stars](https://img.shields.io/github/stars/prompt-security/ps-fuzz)
- [PyRIT](https://github.com/Azure/PyRIT) ![Stars](https://img.shields.io/github/stars/Azure/PyRIT)
- [promptmap](https://github.com/utkusen/promptmap) ![Stars](https://img.shields.io/github/stars/utkusen/promptmap)
- [rebuff](https://github.com/protectai/rebuff) ![Stars](https://img.shields.io/github/stars/protectai/rebuff)
- [TrustGate](https://github.com/NeuralTrust/TrustGate) ![Stars](https://img.shields.io/github/stars/NeuralTrust/TrustGate)
- [VibraniumDome](https://github.com/genia-dev/vibraniumdome) ![Stars](https://img.shields.io/github/stars/genia-dev/vibraniumdome)
- [vigil-llm](https://github.com/deadbits/vigil-llm) ![Stars](https://img.shields.io/github/stars/deadbits/vigil-llm)
  
### LLM02: [Insecure Output Handling](https://genai.owasp.org/llmrisk/llm02-insecure-output-handling/)
- [llm-guard](https://github.com/protectai/llm-guard) ![Stars](https://img.shields.io/github/stars/protectai/llm-guard)
- [PurpleLlama](https://github.com/facebookresearch/PurpleLlama) ![Stars](https://img.shields.io/github/stars/facebookresearch/PurpleLlama)
- [VibraniumDome](https://github.com/genia-dev/vibraniumdome) ![Stars](https://img.shields.io/github/stars/genia-dev/vibraniumdome)

### LLM03: [Training Data Poisoning](https://genai.owasp.org/llmrisk/llm03-training-data-poisoning/)
### LLM04: [Model Denial of Service](https://genai.owasp.org/llmrisk/llm04-model-denial-of-service/)
### LLM05: [Supply Chain Vulnerabilities](https://genai.owasp.org/llmrisk/llm05-supply-chain-vulnerabilities/)
- [modelscan](https://github.com/protectai/modelscan) ![Stars](https://img.shields.io/github/stars/protectai/modelscan)

### LLM06: [Sensitive Information Disclosure](https://genai.owasp.org/llmrisk/llm06-sensitive-information-disclosure/)
- [fast-llm-security](https://github.com/ZenGuard-AI/fast-llm-security) ![Stars](https://img.shields.io/github/stars/ZenGuard-AI/fast-llm-security)
- [Garak](https://github.com/leondz/garak/) ![Stars](https://img.shields.io/github/stars/leondz/garak)
- [llm-security](https://github.com/greshake/llm-security) ![Stars](https://img.shields.io/github/stars/greshake/llm-security)
- [plexiglass](https://github.com/safellama/plexiglass) ![Stars](https://img.shields.io/github/stars/safellama/plexiglass)
- [VibraniumDome](https://github.com/genia-dev/vibraniumdome) ![Stars](https://img.shields.io/github/stars/genia-dev/vibraniumdome)

### LLM07: [Insecure Plugin Design](https://genai.owasp.org/llmrisk/llm07-insecure-plugin-design/)
### LLM08: [Excessive Agency](https://genai.owasp.org/llmrisk/llm08-excessive-agency/)
### LLM09: [Overreliance](https://genai.owasp.org/llmrisk/llm09-overreliance/)
### LLM10: [Model Theft](https://genai.owasp.org/llmrisk/llm10-model-theft/)

## Resources
- [awesome-llm-security](https://github.com/corca-ai/awesome-llm-security) ![Stars](https://img.shields.io/github/stars/corca-ai/awesome-llm-security) -  A curation of awesome tools, documents and projects about LLM Security. 
- [llm-security](https://github.com/llmsecnet/llmsec-site) ![Stars](https://img.shields.io/github/stars/llmsecnet/llmsec-site) - https://llmsecurity.net/ - large language model security content - research, papers, and news
